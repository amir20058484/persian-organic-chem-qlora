{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "Install the required libraries to load the base model, run 4-bit quantization on GPU, fine-tune with LoRA (PEFT), and handle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:23:04.190777Z",
     "iopub.status.busy": "2025-12-28T20:23:04.190548Z",
     "iopub.status.idle": "2025-12-28T20:23:23.260417Z",
     "shell.execute_reply": "2025-12-28T20:23:23.259739Z",
     "shell.execute_reply.started": "2025-12-28T20:23:04.190755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install -U transformers accelerate bitsandbytes\n",
    "!pip -q install -U transformers accelerate bitsandbytes peft datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json, re, torch\n",
    "from peft import PeftModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "Read the JSON file from the Kaggle input path, load it into memory, and quickly verify the dataset size and schema (keys of the first record).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-28T20:23:04.129632Z",
     "iopub.status.busy": "2025-12-28T20:23:04.128965Z",
     "iopub.status.idle": "2025-12-28T20:23:04.147659Z",
     "shell.execute_reply": "2025-12-28T20:23:04.146639Z",
     "shell.execute_reply.started": "2025-12-28T20:23:04.129602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 100\n",
      "First keys: ['question', 'options', 'correct', 'reasoning']\n"
     ]
    }
   ],
   "source": [
    "path = Path(\"/kaggle/input/chapter1/carey_sundberg_partA_ch1_100_mcq_fa.json\")\n",
    "\n",
    "data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "print(\"Loaded:\", len(data))\n",
    "print(\"First keys:\", list(data[0].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split (80/20)\n",
    "Shuffle the dataset with a fixed random seed, then split it into 80% training and 20% testing to evaluate the model before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:23:04.148837Z",
     "iopub.status.busy": "2025-12-28T20:23:04.148606Z",
     "iopub.status.idle": "2025-12-28T20:23:04.162858Z",
     "shell.execute_reply": "2025-12-28T20:23:04.162177Z",
     "shell.execute_reply.started": "2025-12-28T20:23:04.148808Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 80  Test: 20\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "rng = random.Random(seed)\n",
    "\n",
    "idx = list(range(len(data)))\n",
    "rng.shuffle(idx)\n",
    "\n",
    "n_test = int(0.2 * len(data)) \n",
    "test_idx = set(idx[:n_test])\n",
    "\n",
    "train = [data[i] for i in range(len(data)) if i not in test_idx]\n",
    "test  = [data[i] for i in range(len(data)) if i in test_idx]\n",
    "\n",
    "print(\"Train:\", len(train), \" Test:\", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save split datasets\n",
    "Write the 80/20 split into `train.json` and `test.json` in the Kaggle working directory so they can be reused for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:23:04.167197Z",
     "iopub.status.busy": "2025-12-28T20:23:04.165758Z",
     "iopub.status.idle": "2025-12-28T20:23:04.187712Z",
     "shell.execute_reply": "2025-12-28T20:23:04.187166Z",
     "shell.execute_reply.started": "2025-12-28T20:23:04.167169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /kaggle/working/train.json /kaggle/working/test.json\n"
     ]
    }
   ],
   "source": [
    "out_train = Path(\"/kaggle/working/train.json\")\n",
    "out_test  = Path(\"/kaggle/working/test.json\")\n",
    "\n",
    "out_train.write_text(json.dumps(train, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "out_test.write_text(json.dumps(test, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\", out_train, out_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the base model (4-bit)\n",
    "Load the Qwen2.5-3B-Instruct model with 4-bit NF4 quantization to reduce GPU memory usage, then initialize the tokenizer and set the model to evaluation mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:23:23.261856Z",
     "iopub.status.busy": "2025-12-28T20:23:23.261605Z",
     "iopub.status.idle": "2025-12-28T20:24:38.730893Z",
     "shell.execute_reply": "2025-12-28T20:24:38.730135Z",
     "shell.execute_reply.started": "2025-12-28T20:23:23.261828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4944e7b1e5b044119a822d0ffc7b5ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9096952de56423ca702ae5a48876cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422cd008f3514f9e83cae04fa7b8c0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e5b65619ba4450b9934183573a02af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c54cf6a3b3415dbfb61d2e81c3c65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-28 20:23:36.619085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766953416.770698      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766953416.814906      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766953417.170908      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766953417.170940      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766953417.170943      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766953417.170945      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17def58e1a034fbfaffd9c0df1a4ae1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d99a427c2948c3852d1718b76c9c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5521f5ae914fb3b2de0c1c291114b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07190f4b5fe466cb9e73609ccde819b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d98a30b95d42f08e80c42762185893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75d717c55c34aadbd2102674b7102a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Qwen/Qwen2.5-3B-Instruct\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  \n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded:\", MODEL_NAME)\n",
    "print(\"Device:\", model.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline inference (single example)\n",
    "Run the base model on one test question using a strict multiple-choice prompt, then parse the output to extract a single letter (A/B/C/D) and compare it with the ground-truth answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:24:38.732522Z",
     "iopub.status.busy": "2025-12-28T20:24:38.731917Z",
     "iopub.status.idle": "2025-12-28T20:24:40.017959Z",
     "shell.execute_reply": "2025-12-28T20:24:40.017343Z",
     "shell.execute_reply.started": "2025-12-28T20:24:38.732489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: None\n",
      "Gold: B\n",
      "Model raw output: ' D\\nD) '\n",
      "Parsed: D\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test = json.load(open(\"/kaggle/working/test.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "def build_prompt(item):\n",
    "    opts = []\n",
    "    for o in item[\"options\"]:\n",
    "        opts.append(o.replace(\"A:\", \"A)\").replace(\"B:\", \"B)\").replace(\"C:\", \"C)\").replace(\"D:\", \"D)\"))\n",
    "    return (\n",
    "        \"یک سوال چهارگزینه‌ای شیمی آلی به زبان فارسی داریم.\\n\"\n",
    "        \"فقط حرف گزینهٔ صحیح را از بین A/B/C/D برگردان و هیچ متن دیگری ننویس.\\n\\n\"\n",
    "        f\"سوال:\\n{item['question']}\\n\\n\"\n",
    "        f\"گزینه‌ها:\\n\" + \"\\n\".join(opts) + \"\\n\\n\"\n",
    "        \"پاسخ:\"\n",
    "    )\n",
    "\n",
    "def extract_choice(text):\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", text.upper())\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_one(item):\n",
    "    prompt = build_prompt(item)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    gen = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return gen, extract_choice(gen)\n",
    "\n",
    "ex = test[5]\n",
    "gen, pred = predict_one(ex)\n",
    "\n",
    "print(\"ID:\", ex.get(\"id\"))\n",
    "print(\"Gold:\", ex[\"correct\"])\n",
    "print(\"Model raw output:\", repr(gen))\n",
    "print(\"Parsed:\", pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline evaluation (full test set)\n",
    "Evaluate the base model on all test questions, compute baseline accuracy, track any outputs that fail to parse into A/B/C/D, and print a few incorrect examples for quick error analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:24:40.019076Z",
     "iopub.status.busy": "2025-12-28T20:24:40.018856Z",
     "iopub.status.idle": "2025-12-28T20:25:02.802353Z",
     "shell.execute_reply": "2025-12-28T20:25:02.801748Z",
     "shell.execute_reply.started": "2025-12-28T20:24:40.019054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 20\n",
      "Baseline Accuracy: 0.75\n",
      "No-parse outputs: 0\n",
      "Wrong: 5\n",
      "--------------------------------------------------------------------------------\n",
      "id: None\n",
      "gold: A  pred: B\n",
      "raw output: 'B\\n\\nپاسخ'\n",
      "--------------------------------------------------------------------------------\n",
      "id: None\n",
      "gold: A  pred: B\n",
      "raw output: 'B\\nB) آ'\n",
      "--------------------------------------------------------------------------------\n",
      "id: None\n",
      "gold: B  pred: D\n",
      "raw output: 'D\\nD)'\n",
      "--------------------------------------------------------------------------------\n",
      "id: None\n",
      "gold: C  pred: D\n",
      "raw output: 'D) افزای'\n",
      "--------------------------------------------------------------------------------\n",
      "id: None\n",
      "gold: A  pred: B\n",
      "raw output: 'B\\nB) ا'\n"
     ]
    }
   ],
   "source": [
    "TEST_PATH = \"/kaggle/working/test.json\"\n",
    "\n",
    "test = json.load(open(TEST_PATH, \"r\", encoding=\"utf-8\"))\n",
    "print(\"Test size:\", len(test))\n",
    "\n",
    "def build_prompt(item):\n",
    "    opts = []\n",
    "    for o in item[\"options\"]:\n",
    "        opts.append(o.replace(\"A:\", \"A)\").replace(\"B:\", \"B)\").replace(\"C:\", \"C)\").replace(\"D:\", \"D)\"))\n",
    "    return (\n",
    "        \"یک سوال چهارگزینه‌ای شیمی آلی به زبان فارسی داریم.\\n\"\n",
    "        \"فقط حرف گزینهٔ صحیح را از بین A/B/C/D برگردان و هیچ متن دیگری ننویس.\\n\\n\"\n",
    "        f\"سوال:\\n{item['question']}\\n\\n\"\n",
    "        f\"گزینه‌ها:\\n\" + \"\\n\".join(opts) + \"\\n\\n\"\n",
    "        \"پاسخ:\"\n",
    "    )\n",
    "\n",
    "def extract_choice(text):\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", text.upper())\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_choice(item):\n",
    "    prompt = build_prompt(item)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    gen = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "    return gen, extract_choice(gen)\n",
    "\n",
    "correct = 0\n",
    "none_count = 0\n",
    "rows = [] \n",
    "\n",
    "for ex in test:\n",
    "    raw, pred = predict_choice(ex)\n",
    "    gold = ex[\"correct\"]\n",
    "    ok = (pred == gold)\n",
    "    if pred is None:\n",
    "        none_count += 1\n",
    "    if ok:\n",
    "        correct += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": ex.get(\"id\"),\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"raw\": raw,\n",
    "        \"is_correct\": ok\n",
    "    })\n",
    "\n",
    "acc = correct / len(test)\n",
    "print(\"Baseline Accuracy:\", acc)\n",
    "print(\"No-parse outputs:\", none_count)\n",
    "\n",
    "wrong = [r for r in rows if not r[\"is_correct\"]]\n",
    "print(\"Wrong:\", len(wrong))\n",
    "for r in wrong[:10]:\n",
    "    print(\"-\"*80)\n",
    "    print(\"id:\", r[\"id\"])\n",
    "    print(\"gold:\", r[\"gold\"], \" pred:\", r[\"pred\"])\n",
    "    print(\"raw output:\", repr(r[\"raw\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the SFT training file (prompt + answer)\n",
    "Convert `train.json` into a JSONL file where each record contains a single `text` field: a formatted multiple-choice prompt followed by the correct letter. This format is used for supervised fine-tuning (SFT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.803533Z",
     "iopub.status.busy": "2025-12-28T20:25:02.803262Z",
     "iopub.status.idle": "2025-12-28T20:25:02.813958Z",
     "shell.execute_reply": "2025-12-28T20:25:02.813263Z",
     "shell.execute_reply.started": "2025-12-28T20:25:02.803510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 80\n",
      "Saved: /kaggle/working/train_sft.jsonl\n",
      "\n",
      "--- sample ---\n",
      " ### دستورالعمل:\n",
      "یک سوال چهارگزینه‌ای شیمی آلی به زبان فارسی داریم. فقط حرف گزینهٔ صحیح را از بین A/B/C/D برگردان و هیچ متن دیگری ننویس.\n",
      "\n",
      "### سوال:\n",
      "در نظریه اوربیتال مولکولی، برهم‌کنش سازنده (هم‌فاز) بین دو اوربیتال اتمی معمولاً چه نتیجه‌ای دارد؟\n",
      "\n",
      "### گزینه‌ها:\n",
      "A) ایجاد اوربیتال پیوندی با انرژی کمتر از اوربیتال‌های اتمی\n",
      "B) ایجاد اوربیتال ضدپیوندی با انرژی کمتر\n",
      "C) ایجاد اوربیتال ناپیوندی با انرژی بسیار بالاتر\n",
      "D) حذف کامل هم‌پوشانی و بدون تغییر انرژی\n",
      "\n",
      "### پاسخ:\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = Path(\"/kaggle/working/train.json\")\n",
    "OUT_PATH = Path(\"/kaggle/working/train_sft.jsonl\")\n",
    "\n",
    "train = json.loads(TRAIN_PATH.read_text(encoding=\"utf-8\"))\n",
    "print(\"Train size:\", len(train))\n",
    "\n",
    "def build_text(item):\n",
    "    opts = \"\\n\".join([\n",
    "        o.replace(\"A:\", \"A)\").replace(\"B:\", \"B)\").replace(\"C:\", \"C)\").replace(\"D:\", \"D)\")\n",
    "        for o in item[\"options\"]\n",
    "    ])\n",
    "    prompt = (\n",
    "        \"### دستورالعمل:\\n\"\n",
    "        \"یک سوال چهارگزینه‌ای شیمی آلی به زبان فارسی داریم. فقط حرف گزینهٔ صحیح را از بین A/B/C/D برگردان و هیچ متن دیگری ننویس.\\n\\n\"\n",
    "        \"### سوال:\\n\"\n",
    "        f\"{item['question'].strip()}\\n\\n\"\n",
    "        \"### گزینه‌ها:\\n\"\n",
    "        f\"{opts}\\n\\n\"\n",
    "        \"### پاسخ:\\n\"\n",
    "    )\n",
    "    return prompt + item[\"correct\"].strip()\n",
    "\n",
    "OUT_PATH.write_text(\n",
    "    \"\\n\".join(json.dumps({\"id\": x.get(\"id\",\"\"), \"text\": build_text(x)}, ensure_ascii=False) for x in train),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"Saved:\", OUT_PATH)\n",
    "\n",
    "sample = json.loads(OUT_PATH.read_text(encoding=\"utf-8\").splitlines()[0])\n",
    "print(\"\\n--- sample ---\\n\", sample[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the training data\n",
    "Load the SFT JSONL file, tokenize each training example (with truncation to a maximum sequence length), and create a causal language modeling data collator for efficient batching during fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.815644Z",
     "iopub.status.busy": "2025-12-28T20:25:02.814849Z",
     "iopub.status.idle": "2025-12-28T20:25:09.919475Z",
     "shell.execute_reply": "2025-12-28T20:25:09.918507Z",
     "shell.execute_reply.started": "2025-12-28T20:25:02.815621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1349e12b8474fce9d13b5d33ed0085f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train records: 80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2fff6c53554c0eb12f9773f449bbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "ds = load_dataset(\"json\", data_files={\"train\": \"/kaggle/working/train_sft.jsonl\"})[\"train\"]\n",
    "print(\"train records:\", len(ds))\n",
    "\n",
    "MAX_LEN = 512 \n",
    "\n",
    "def tok(ex):\n",
    "    return tokenizer(ex[\"text\"], truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "tok_ds = ds.map(tok, remove_columns=ds.column_names)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune with QLoRA (LoRA adapters)\n",
    "Attach LoRA adapters to the 4-bit base model and fine-tune only these lightweight parameters using the tokenized training set. Save the trained adapter for later evaluation and reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:09.922625Z",
     "iopub.status.busy": "2025-12-28T20:25:09.921568Z",
     "iopub.status.idle": "2025-12-28T20:27:07.089442Z",
     "shell.execute_reply": "2025-12-28T20:27:07.088762Z",
     "shell.execute_reply.started": "2025-12-28T20:25:09.922593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,966,784 || all params: 3,100,905,472 || trainable%: 0.4827\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 01:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.697500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved adapter to /kaggle/working/carey_lora_adapter_v2\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.10,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/carey_qwen3b_qlora_run_v2\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,      \n",
    "    gradient_accumulation_steps=16,     \n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_ds,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"/kaggle/working/carey_lora_adapter_v2\")\n",
    "print(\"Saved adapter to /kaggle/working/carey_lora_adapter_v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post fine-tuning evaluation\n",
    "Run the fine-tuned model on the full test set using the same multiple-choice prompt format, extract the predicted option (A/B/C/D), and compute the final accuracy to compare against the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:27:07.101479Z",
     "iopub.status.busy": "2025-12-28T20:27:07.101149Z",
     "iopub.status.idle": "2025-12-28T20:27:25.463919Z",
     "shell.execute_reply": "2025-12-28T20:27:25.463245Z",
     "shell.execute_reply.started": "2025-12-28T20:27:07.101426Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 20\n",
      "Accuracy after fine-tune: 0.85\n"
     ]
    }
   ],
   "source": [
    "TEST_PATH = \"/kaggle/working/test.json\"\n",
    "ADAPTER_PATH = \"/kaggle/working/carey_lora_adapter\"\n",
    "\n",
    "test = json.load(open(TEST_PATH, \"r\", encoding=\"utf-8\"))\n",
    "print(\"Test size:\", len(test))\n",
    "\n",
    "def build_prompt(item):\n",
    "    opts = []\n",
    "    for o in item[\"options\"]:\n",
    "        opts.append(o.replace(\"A:\", \"A)\").replace(\"B:\", \"B)\").replace(\"C:\", \"C)\").replace(\"D:\", \"D)\"))\n",
    "    return (\n",
    "        \"یک سوال چهارگزینه‌ای شیمی آلی به زبان فارسی داریم.\\n\"\n",
    "        \"فقط حرف گزینهٔ صحیح را از بین A/B/C/D برگردان و هیچ متن دیگری ننویس.\\n\\n\"\n",
    "        f\"سوال:\\n{item['question']}\\n\\n\"\n",
    "        f\"گزینه‌ها:\\n\" + \"\\n\".join(opts) + \"\\n\\n\"\n",
    "        \"پاسخ:\"\n",
    "    )\n",
    "\n",
    "def extract_choice(text):\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", text.upper())\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(item):\n",
    "    prompt = build_prompt(item)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=5, do_sample=False, temperature=0.0)\n",
    "    gen = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "    return extract_choice(gen)\n",
    "\n",
    "correct = 0\n",
    "for ex in test:\n",
    "    if predict(ex) == ex[\"correct\"]:\n",
    "        correct += 1\n",
    "\n",
    "acc_after = correct / len(test)\n",
    "print(\"Accuracy after fine-tune:\", acc_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final report (summary)\n",
    "\n",
    "In this project, we first generated a Persian multiple-choice (MCQ) dataset based on *Advanced Organic Chemistry (Carey)* using a large language model, then manually reviewed and standardized the questions. Initially, the questions covered the entire book; however, because the dataset was small (100 questions) and the book is very broad, the coverage per topic/chapter was insufficient and the fine-tuning improvements were limited. Therefore, we narrowed the scope to a single chapter to make the training data more focused and consistent.\n",
    "\n",
    "Next, we split the dataset into **80% training / 20% testing**, loaded **Qwen2.5-3B-Instruct** in **4-bit** mode, and measured baseline performance. We then fine-tuned the model using **QLoRA/LoRA** on the training set and evaluated it again on the held-out test set.\n",
    "\n",
    "Key challenges included **TRL/SFTTrainer version incompatibilities on Kaggle** (causing repeated argument errors) and the small test set size (20 questions), which makes evaluation more sensitive to small changes. After switching to the stable **Transformers Trainer** workflow and focusing on one chapter, the fine-tuned model improved from **0.75 accuracy before fine-tuning to 0.85 after fine-tuning** (+0.10).\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9128937,
     "sourceId": 14300777,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9144553,
     "sourceId": 14324226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
